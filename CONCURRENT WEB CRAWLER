import re
import sys
import time
import queue
import threading
import urllib.parse
import urllib.request

HREF_RE = re.compile(r'(?i)href\s*=\s*(?:"([^"]*)"|\'([^\']*)\'|([^\s>]+))')
ABS_HTTP_RE = re.compile(r'(?i)^https?://')

class Crawler:
    def __init__(self, seed_url, max_depth=2, max_pages=100, threads=8, host_delay=0.4):
        self.seed_url = seed_url
        self.max_depth = max_depth
        self.max_pages = max_pages
        self.threads = threads
        self.host_delay = host_delay
        self.visited = set()
        self.jobs = queue.Queue()
        self.pages_fetched = 0
        self.host_last = {}
        self.lock = threading.Lock()
        self.stop = False

    def normalize(self, raw, base=None):
        raw = raw.strip()
        if not ABS_HTTP_RE.match(raw):
            if base is None:
                return None
            raw = urllib.parse.urljoin(base, raw)
        parsed = urllib.parse.urlparse(raw)
        if parsed.scheme not in ('http', 'https'):
            return None
        # strip fragment
        parsed = parsed._replace(fragment='')
        url = urllib.parse.urlunparse(parsed)
        if url.endswith('/'):
            url = url[:-1]
        return url

    def throttle(self, url):
        host = urllib.parse.urlparse(url).netloc
        now = time.time()
        last = self.host_last.get(host, 0.0)
        wait = self.host_delay - (now - last)
        if wait > 0:
            time.sleep(wait)
        self.host_last[host] = time.time()

    def fetch(self, url, timeout=10):
        try:
            req = urllib.request.Request(url, headers={'User-Agent': 'PyCrawler/1.0'})
            with urllib.request.urlopen(req, timeout=timeout) as resp:
                if 200 <= resp.status < 300:
                    charset = resp.headers.get_content_charset() or 'utf-8'
                    return resp.read().decode(charset, errors='replace')
                return None
        except Exception:
            return None

    def extract_links(self, base_url, html):
        links = []
        for m in HREF_RE.finditer(html):
            raw = m.group(1) or m.group(2) or m.group(3) or ''
            url = self.normalize(raw, base_url)
            if url:
                links.append(url)
        return links

    def worker(self):
        while not self.stop:
            try:
                url, depth = self.jobs.get(timeout=0.5)
            except queue.Empty:
                continue
            with self.lock:
                if url in self.visited or self.pages_fetched >= self.max_pages:
                    self.jobs.task_done()
                    continue
                self.visited.add(url)
            self.throttle(url)
            body = self.fetch(url)
            ok = body is not None
            with self.lock:
                if ok:
                    self.pages_fetched += 1
                    print(f"[OK {self.pages_fetched:4d}] depth={depth} url={url}")
                else:
                    print(f"[FAIL] url={url}")
            if ok and depth < self.max_depth and self.pages_fetched < self.max_pages:
                for link in self.extract_links(url, body):
                    with self.lock:
                        if link not in self.visited:
                            self.jobs.put((link, depth + 1))
            self.jobs.task_done()

    def start(self):
        seed = self.normalize(self.seed_url)
        if not seed:
            print("Invalid seed URL")
            return
        self.jobs.put((seed, 0))
        threads = [threading.Thread(target=self.worker, daemon=True) for _ in range(self.threads)]
        for t in threads:
            t.start()
        try:
            while self.pages_fetched < self.max_pages and any(t.is_alive() for t in threads):
                time.sleep(0.2)
                if self.jobs.empty() and self.pages_fetched >= self.max_pages:
                    break
        finally:
            self.stop = True
            for t in threads:
                t.join(timeout=1.0)
        print("Crawl finished.")
        print(f"Pages fetched: {self.pages_fetched}")
        print(f"Unique visited: {len(self.visited)}")

if __name__ == "__main__":
    if len(sys.argv) < 5:
        print("Usage: python crawler.py <seedUrl> <maxDepth> <maxPages> <threads>")
        sys.exit(1)
    seed = sys.argv[1]
    max_depth = int(sys.argv[2])
    max_pages = int(sys.argv[3])
    threads = int(sys.argv[4])
    Crawler(seed, max_depth, max_pages, threads).start()
